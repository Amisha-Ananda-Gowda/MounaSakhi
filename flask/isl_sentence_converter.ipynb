{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: spacy in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (3.8.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from spacy) (1.0.11)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from spacy) (8.3.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from spacy) (0.14.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from spacy) (2.10.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.1.0,>=1.0.0 in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.0.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\lenovo\\onedrive\\desktop\\mounasakhi\\mounasakhi\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 11.2 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 3.9/12.8 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 6.3/12.8 MB 10.7 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 8.4/12.8 MB 10.6 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 10.5/12.8 MB 10.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.6/12.8 MB 10.4 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 10.0 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed ISL-Compatible Tokens: ['I', 'python', 'program', 'like']\n",
      "--------------------------------------------------\n",
      "Processed ISL-Compatible Tokens: ['I', 'ashriya']\n",
      "--------------------------------------------------\n",
      "Processed ISL-Compatible Tokens: ['I', 'ashriya']\n",
      "--------------------------------------------------\n",
      "Processed ISL-Compatible Tokens: ['live', 'mangalore', 'I']\n",
      "--------------------------------------------------\n",
      "Processed ISL-Compatible Tokens: ['she', 'dance', 'like']\n",
      "--------------------------------------------------\n",
      "Processed ISL-Compatible Tokens: ['she', 'girl', 'tall']\n",
      "--------------------------------------------------\n",
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Tokenize and lemmatize function\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())  # Tokenize text\n",
    "    processed_tokens = []\n",
    "\n",
    "    # Lemmatize each word and remove stopwords\n",
    "    for token in tokens:\n",
    "        if token not in stop_words and token.isalpha():\n",
    "            processed_tokens.append(lemmatizer.lemmatize(token))\n",
    "\n",
    "    return processed_tokens\n",
    "\n",
    "\n",
    "# for normal sentences = subject+object+verb\n",
    "\n",
    "def process_text_isl(text):\n",
    "    \"\"\"\n",
    "    Processes the input text by tokenizing, removing stopwords,\n",
    "    lemmatizing, and reordering based on ISL grammar rules.\n",
    "    \"\"\"\n",
    "    # Tokenize and lemmatize\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if token.lemma_ not in stop_words]\n",
    "\n",
    "    # Apply ISL grammar: Example SOV structure adjustment\n",
    "    # Example: \"I am learning sign language\" -> \"learning sign language I\"\n",
    "    if len(tokens) >= 3:  # Check if there are enough words to rearrange\n",
    "        # Assume format [Subject, Verb, Object] becomes [Object, Subject, Verb]\n",
    "        isl_tokens = tokens[1:] + [tokens[0]]\n",
    "    else:\n",
    "        isl_tokens = tokens  # For short phrases, no change\n",
    "\n",
    "    # Return the processed list of words\n",
    "    return isl_tokens\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#subject+object+question words\n",
    "\n",
    "def process_question_isl(text):\n",
    "    \"\"\"\n",
    "    Processes question sentences by reordering based on ISL grammar rules.\n",
    "    Converts [Subject-Verb-Object] to [Object-Subject-Verb] and moves\n",
    "    the question word to the end (e.g., 'What is your name?' -> 'your name what').\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Tokenize the input text and identify key components\n",
    "    tokens = [token.text for token in doc if not token.is_punct]\n",
    "\n",
    "    # Identify the question word (e.g., who, what, where, etc.)\n",
    "    question_word = None\n",
    "    if tokens[0].lower() in {\"who\", \"what\", \"where\", \"when\", \"why\", \"how\"}:\n",
    "        question_word = tokens.pop(0)  # Remove the question word from the front\n",
    "\n",
    "    # Filter auxiliary verbs\n",
    "    tokens = [token for token in tokens if token.lower() not in {\"is\", \"are\", \"be\"}]\n",
    "\n",
    "    # Reorder tokens to follow ISL grammar rules\n",
    "    if len(tokens) >= 2:\n",
    "        isl_tokens = tokens + ([question_word] if question_word else [])\n",
    "    else:\n",
    "        isl_tokens = tokens  # If too short, no reordering\n",
    "\n",
    "    return isl_tokens\n",
    "\n",
    "\n",
    "\n",
    "#subject+object+ +ve and -ve words\n",
    "\n",
    "\n",
    "\n",
    "def process_to_isl(text):\n",
    "    \"\"\"\n",
    "    Converts an English sentence to an ISL-compatible structure:\n",
    "    - Subject + Object + Verb (\"like\") (+ Negation if present).\n",
    "    - Handles proper dependency parsing for subject, object, and negation.\n",
    "    \"\"\"\n",
    "    # Tokenize the sentence\n",
    "    doc = nlp(text)\n",
    "\n",
    "    subject = None\n",
    "    object_ = None\n",
    "    action = None\n",
    "    negation = False\n",
    "\n",
    "    for token in doc:\n",
    "        if token.dep_ == \"nsubj\":  # Subject\n",
    "            subject = token.text\n",
    "        elif token.dep_ in {\"dobj\", \"pobj\"}:  # Object\n",
    "            object_ = token.text\n",
    "        elif token.dep_ == \"neg\":  # Negation\n",
    "            negation = True\n",
    "        elif token.pos_ == \"VERB\":  # Verb\n",
    "            # Identify action for ISL (remove 's' for singular verbs like 'likes')\n",
    "            action = token.lemma_ if token.lemma_ != \"like\" else None\n",
    "\n",
    "    # Build ISL-compatible sentence as a list of tokens\n",
    "    isl_tokens = []\n",
    "    if subject:\n",
    "        isl_tokens.append(subject)\n",
    "    if object_:\n",
    "        isl_tokens.append(object_)\n",
    "    if action:\n",
    "        isl_tokens.append(action)\n",
    "    isl_tokens.append(\"like\")\n",
    "    if negation:\n",
    "        isl_tokens.append(\"not\")\n",
    "\n",
    "    return isl_tokens\n",
    "\n",
    "\n",
    "# subject+object+number\n",
    "def process_subject_object_number_sentence(text):\n",
    "    \"\"\"\n",
    "    Processes sentences with subject + object + number into ISL format:\n",
    "    - For sentences like 'I have 2 brothers', convert to 'I brothers 2'.\n",
    "    - For sentences like 'I have 2 apples', convert to 'I 2 apples'.\n",
    "    \"\"\"\n",
    "    # Tokenize the sentence using spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Initialize variables to store subject, object, and number\n",
    "    subject = None\n",
    "    object_ = None\n",
    "    number = None\n",
    "\n",
    "    # Iterate through the tokens to extract subject, object, and number\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"PRON\" or token.pos_ == \"PROPN\":  # Pronouns or proper nouns are subject\n",
    "            if not subject:\n",
    "                subject = token.text\n",
    "        elif token.pos_ == \"NOUN\":  # Nouns are the object\n",
    "            if not object_:\n",
    "                object_ = token.text\n",
    "        elif token.pos_ == \"NUM\":  # Numbers\n",
    "            number = token.text\n",
    "\n",
    "    # Handle reordering for ISL: subject + number + object\n",
    "    isl_tokens = []\n",
    "\n",
    "    if subject:\n",
    "        isl_tokens.append(subject)  # Add the subject first\n",
    "    if number:\n",
    "        isl_tokens.append(number)  # Add the number next\n",
    "    if object_:\n",
    "        isl_tokens.append(object_)  # Add the object last\n",
    "\n",
    "    return isl_tokens\n",
    "\n",
    "\n",
    "\n",
    "#subject+object+adjective\n",
    "def process_adjective_sentence(text):\n",
    "    \"\"\"\n",
    "    Converts an English sentence with adjectives to an ISL-compatible structure:\n",
    "    - Subject + Object + Adjective (+ Negation if present).\n",
    "    - Returns individual tokens as a list.\n",
    "    \"\"\"\n",
    "    # Tokenize the sentence\n",
    "    doc = nlp(text)\n",
    "\n",
    "    subject = None\n",
    "    object_ = None\n",
    "    adjective = None\n",
    "    negation = False\n",
    "\n",
    "    for token in doc:\n",
    "        # Identifying subject and object based on dependency parsing\n",
    "        if token.dep_ == \"nsubj\":  # Subject\n",
    "            subject = token.text\n",
    "        elif token.dep_ in {\"dobj\", \"pobj\"}:  # Object\n",
    "            object_ = token.text\n",
    "        elif token.dep_ == \"neg\":  # Negation\n",
    "            negation = True\n",
    "        elif token.pos_ == \"ADJ\":  # Adjective\n",
    "            adjective = token.text\n",
    "\n",
    "    # Handle the case where object is not captured from dependencies\n",
    "    if not object_:\n",
    "        for token in doc:\n",
    "            # Sometimes the object may not be captured properly, we check for common nouns (e.g., 'boy', 'girl')\n",
    "            if token.pos_ == \"NOUN\" and token.dep_ != \"nsubj\":\n",
    "                object_ = token.text\n",
    "                break\n",
    "\n",
    "    # Build ISL-compatible sentence as a list of tokens\n",
    "    isl_tokens = []\n",
    "    if subject:\n",
    "        isl_tokens.append(subject)\n",
    "    if object_:\n",
    "        isl_tokens.append(object_)\n",
    "    if adjective:\n",
    "        isl_tokens.append(adjective)\n",
    "    if negation:\n",
    "        isl_tokens.append(\"not\")\n",
    "\n",
    "    return isl_tokens\n",
    "\n",
    "\n",
    "# Function to determine the logic based on sentence type\n",
    "# Function to determine the logic based on sentence type\n",
    "def process_input_sentence(sentence):\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    # Check for question sentences\n",
    "    if \"?\" in sentence:\n",
    "        return process_question_isl(sentence)\n",
    "\n",
    "    # Check for sentences with positive/negative words\n",
    "    if any(word in sentence.lower() for word in [\"like\", \"dislike\", \"love\", \"hate\", \"not\", \"doesn't\", \"don't\"]):\n",
    "        return process_to_isl(sentence)\n",
    "\n",
    "    # Check for sentences with numbers\n",
    "    if any(token.pos_ == \"NUM\" for token in doc):\n",
    "        return process_subject_object_number_sentence(sentence)\n",
    "\n",
    "    # Check for sentences with adjectives\n",
    "    if any(token.pos_ == \"ADJ\" for token in doc):\n",
    "        return process_adjective_sentence(sentence)\n",
    "\n",
    "    # Fallback to normal text processing for unclassified sentences\n",
    "    return process_text_isl(sentence)\n",
    "\n",
    "# Main function for interactive input\n",
    "def interactive_test():\n",
    "    while True:\n",
    "        user_input = input(\"Input: \").strip()\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "        try:\n",
    "            result = process_input_sentence(user_input)\n",
    "            print(\"Processed ISL-Compatible Tokens:\", result)\n",
    "        except Exception as e:\n",
    "            print(\"Error processing the input:\", str(e))\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Run the function\n",
    "interactive_test()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
